\chapter{Systementwurf und Implementierung}

Zusammenfassung der Vorlesung "`Systementwurf und Implementierung"' aus dem Sommersemester 2017.\footnote{\url{http://os.itec.kit.edu/deutsch/3321_3327.php}}

\section{Betriebssystemstrukturen und -schnittstellen}

\subsection{Traditionelle Betriebssystemstrukturen}
\begin{itemize}
	\item Monolithischer Kernel im privilegierten Modus
	\item (Multithreaded) Anwendungen im Benutzermodus
	\item Schnittstellen: Library API; Syscalls
\end{itemize}


\subsection{Ziele und Funktionalität}
\begin{itemize}
	\item \textbf{Aufgaben eines OS}
	\begin{itemize}
		\item Ressourcenmanagement und Accounting
		\item Hardware-Abstraktion: Erlaubt geteilte Zustände (Beispiel: Buffer Cache) sowie Zugriffskontrolle
		\item Fehlervermeidung
		\item Isolierung von Anwendungen inklusive Hardware-Mechanismus (Benutzermodus der CPU) zur Programmausführung (Betriebssystem muss nicht mehr jede Instruktion auf Gültigkeit überprüfen \(\rightarrow\) kein Leistungsverlust)
	\end{itemize}
	\item \textbf{Monolithische Kernel}
	\begin{itemize}
		\item Viele, mit Serices verflochtene Abstraktionen (Prozesse, Dateien, Sockets, etc.)
		\item Schutz lediglich zwischen Prozessen und gegenüber dem Kernel (keinerlei Sicherheit innerhalb des Kernels)
		\item Alle Betriebssystemfunktionalität innerhalb des Kernels (Treiber, Netzwerstack, Dateisysteme, etc.)
		\item Historisches: Teilweise Services im Userspace (\texttt{X Server}) oder innerhalb des Kernel implementiert (\texttt{nfsd})
		\item Schnittstellen: Bibliotheksaufrufe; Systemaufrufe und (asynchrone) Signale; In-Kernel-Interfaces
	\end{itemize}
	\item \textbf{Microkernel- und Multiserver-Systeme}
	\begin{itemize}
		\item Ziel: Microkernel stellt lediglich minimale Funktionalität (Adressraum, Threading, IPC) zur Verfügung \(\rightarrow\) minimiert privilegierten Code
		\item Treiber und Services isoliert im Userspace (Bugs haben minimale Auswirkungen)
		\item Kommunikation mittels \texttt{IPC} oder Shared Memory
		\item Kernel-Schnittstellen in Multiserver-Systemen
		\begin{itemize}
			\item Kernel Subsysteme sind "`Anwendungsprogramme"'
			\item Schnittstellenentwurf bekannt von Verteilten Systemen (gleiches Szenario: Interaktion verteilter Komponenten). Probleme: Calling Conventions (beispielsweise Pointer oder Referenzen); Transparenz (lokales "`Look-and-Feel"' gewünscht; Latenz)
			\item Remote Procedure Call (RPC)
			\begin{itemize}
				\item Mittels Stubs formal definierter Funktionsaufruf. Parameter und Return-Werte müssen per IPC kopiert werden
				\item Funktionsweise
				\begin{enumerate}
					\item Client-Stub wird aufgerufen
					\item Client-Stub ordnet Parameter, baut die Nachricht und sendet diese (per Kernel-Aufruf) zum Server
					\item Server-Stub dekodiert die Nachricht und ruft die entsprechende Server-Prozedur auf
					\item Server verarbeitet die Anfrage und gibt sie an den Server-Stub zurück
				\end{enumerate}
				\item Unterschiede zu Verteilten Systemen: Kommunikation deutlich effizienter \(\rightarrow\) Stub-Code hat größen Einfluss auf Verarbeitungsgeschwindigkeit; selbe Hardware/Kernel \(\rightarrow\) einheitliche Datentypen, Endian, etc.
				\item Verwendung von \textit{Interface Definition Languages} (IDLs) zum Beschreiben und automatischen Erzeugen von Schnittstellen-Code
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Structure Design Space}


\subsection{Fallstudien}

\subsubsection{Fallstudie: \texttt{L4Re RPC}}
\begin{itemize}
	\item Capability: Task-lokale Berechtigung um (Id-adressiert) auf ein Object zuzugreifen. Zugriffskontrolle implizit
	\item IPC-Gate übermittelt Aufrufe
\end{itemize}

\subsubsection{Fallstudie: Exokernel}
\begin{itemize}
	\item Motivation: Existierende Betriebssysteme zu unflexibel, umfangreich und ineffizient; Abstraktionen zu allgemein, unpassend für bestimmte Anwendungen, können nicht geändert werden
	\item Ansatz: Minimale Hardware-Abstraktion (Hardware-Ressourcen werden Anwendungen direkt zur Verfügung gestellt); konfigurierbares Ressourcen-Management (Page Table, TLB, Anwendungen können Kernelmodule laden). Betriebssystem lediglich für Schutz/Multiplexen des Ressourcenzugriffs zuständig
	\item Herausforderungen in der Forschung: Sicheres Anpassen der Betriebssystemabstraktionen durch die Anwendungen; Betriebssystemfunktionalität in Libraries (prägte den Begriff "`library OS"')
	\item \textbf{Abstraktionen}
	\begin{itemize}
		\item Allokieren/Multiplexen physischer Ressourcen wie Speicher, CPU, TLB-Einträge. Beinhaltet keine Abstraktion der Hardware
		\item Schützen der Ressourcen durch Capabilities
		\item libOS: Hardwareabstraktionen für einzelnen Konsumenten
	\end{itemize}
	\item \textbf{Schnittstellen}
	\begin{description}
		\item[libOS API:] \texttt{API} komplett anpassbar; \texttt{ABI}-Aufrufe sind \texttt{API}-Funktionsaufrufe (statt System Calls) \(\rightarrow\) weniger beteiligte Sicherheitseben pro Aufruf
		\item[Exokernel:] Schnittstelle zum Exokernel (per System Call) zum Verwalten von physischen Speicherseiten, DMA-Kanälen, IO-Geräte, TLBs, Prozessoren, Interrupt-Behandlung
	\end{description}
\end{itemize}

\subsubsection{Virtualisierung}
\begin{itemize}
	\item Idee: Simulieren virtueller Maschinen mit den selben Hardware-Schnittstellen physischer Maschinen
	\item \textbf{Technologie-Evolution}
	\begin{itemize}
		\item Zunächst: Trap-and-Emulate (seit den frühen 1970gern im \texttt{IBM VM/370})
		\begin{itemize}
			\item Gastbetriebssystem läuft unprivilegiert, privilegierte Instruktionen erzeugen einen \textit{Trap} und werden vom Hyperviser emuliert. Anschließend Rücksprung zur VM
			\item Formale Voraussetzung\footnote{nach Popek und Goldberg (1974)}: Alle sensitiven Instruktionen müssen privilegierte Instruktionen sein, um vom Hyperviser erkannt zu werden. Nicht erfüllt bei \texttt{x86}: Manche Instruktionen verhalten sich je nach CPU-Modus unterschiedlich (beispielsweise \texttt{pushf} und \texttt{popf})
		\end{itemize}
		\item Workarounds für \texttt{x86}
		\begin{description}
			\item[VMWare:] Ersetzen der problematischen Instruktionen durch Binary Translation zur Laufzeit
			\item[Xen:] Gastsystem wird angepasst und "`weis"', dass es virtualisiert ausgeführt wird (Paravirtualisierung). Privilegierte Instruktionen werden durch \textit{Hypercalls} ersetzt \(\rightarrow\) Ansatz vergleichbar mit Exokernel (Multiplexen der Ressourcen, low-level Syscall-Schnittstelle)
		\end{description}
		\item Hardware-seitige Unterstützung für Virtualisierung
		\begin{itemize}
			\item Zwei CPU-Modi zum Ausführen von privilegierten Instruktionen: Physical/Supervisor/Hypervisor und Guest/Virtualized \(\rightarrow\) \textit{Guest Privileged Mode} und \textit{Host Privileged Mode}
			\item Gastbetriebssystem kann selbstständig zum \textit{Guest User Mode} wechseln und Page-Table-Einträge modifizieren
			\item Hypervisor kontrolliert Mapping der Host-Ressourcen
		\end{itemize}
	\end{itemize}
\end{itemize}



\section{CPU und Thread Management}

\subsection{Threads und Prozesse}
\begin{itemize}
	\item Ziel: Verschiedene/unabhängige Befehlsflüsse (unterschiedliche Serveranfragen; Pausieren während auf I/O gewartet wird; QoS; etc.)
	\item \textbf{Thread- und Prozessverwaltung}
	\begin{itemize}
		\item Betriebssystemaufgaben: Verwaltung, Ausführung, Scheduling, Accounting, Ressourcen-Verwaltung
		\item Zustandshaltung pro Thread inklusive \textit{Instruction Pointer} und \textit{Stack Pointer}; Schedulung-/Accounting-Zustand; Speicherzustand; Kommunikationszustand (Wait-Queue, Send-Queue, etc.). Ressourcen (File-Pointer, etc.); Berechtigungen (\texttt{ugo}. Gepeichert im \textit{Thread Control Block} (TCB), dieser kann auch Pointer zum Vorgänger und zum Nachfolger speichern
		\item Basisoperationen % TODO: Unterschied Startup/Resume
		\begin{description}
			\item[Create:] Erstellen (inklusive TCB) und Einfügen in \textit{Ready Queue}
			\item[Startup:] Entfernen aus \textit{Ready Queue}
			\item[Block:] Speichern des Thread-Zustands auf dem Stack; Einfügen in \texttt{Wait Queue} Aktualisieren des Thread-Zustands; \texttt{resume(next\_thread)}
			\item[Signal:] Thread wird von \textit{wait queue} in \textit{ready queue} verschoeben; Aktualisieren des Thread-Zustands
			\item[Resume:] Entfernen des Threads aus \textit{ready queue}; Laden des Registerzustands; mit Ausführung fortfahren (\textit{Instruction Pointer} zeigt auf den entsprechenden Befehl)
			\item[Finish:] Aufräumen (Stack und TCB); Finden und Weiterausführen des nächsten Thread
		\end{description}
		\item Abwägungen
		\begin{itemize}
			\item Leistungsoptimierung
			\begin{itemize}
				\item Stack wird erst beim Startup reserviert, da neue Threads eventuell nie laufen oder die für die Erstellung benötigten Ressourcen dem neuen Thread berechnet werden sollen (Accounting)
				\item Aufwandsreduzierung beim Finden von freiem Speicher durch nutzen von \textit{Free Memory Lists} für Stack und TCB
			\end{itemize}
			\item Synchronisieren von Thread-Datenstrukturen
			\begin{itemize}
				\item Gleichzeitiger Zugriff muss serialisiert werden
				\item Leistungsmetriken: Latenz bei konfliktfreiem Zugriff; Durchsatz (Operationen pro Sekunde)
				\item Implementierungsmöglichkeiten
				\begin{description}
					\item[Single Lock:] Einzelnes Lock für gesamte Datenstruktur \(\rightarrow\) Niedrige Latenz, allerdings limitierter Durchsatz
					\item[Mehrere Locks:] Separates Lock für \textit{ready queue}, \textit{wait queue}, \textit{free lists}, etc. \(\rightarrow\) höhere Latenz, besseres Durchsatz
					\item[(Prozessor-)lokale Free Lists:] Exklusive Allocation Pools pro Prozessor \(\rightarrow\) verringert Zugriffskonflikte beim allokieren von TCBs oder Stacks, allerdings erhöhter Speicherverwaltungsaufwand. Pools müssen eventuell balanciert werden
					\item[Lokale Ready Queues:] Prozessor-lokale \textit{ready queues}. Reduziert Zugriffskonflikte; setzt ggf. Load Balancing (und Synchronisierung während Load Balancing) voraus. Heutzutage meist verwendet
				\end{description}
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Thread Scheduling}
\begin{itemize}
	\item \textbf{Designziel: Trennung zwischen Policy und Mechanismus}
	\begin{description}
		\item[Mechanismus:] Zuordnung (Dispatch), Unterbrechbarkeit (Preemtion), Migration, Abrechnung (Accounting)
		\item[Policy:] Allokation, Budgetierung, Prioritäten, Dienstgüteklassen, Latenzen, etc.
	\end{description}
	\item Distributed/Hierarchical Scheduling: Scheduling umfasst ggf. mehrere Subsysteme und Schichten. Beispielsweise Scheduling von Webserver-Anfragen zu Webserver Threads sowie Scheduling von Webserver Threads zu CPUs
	\item Ggf. nehmen weitere Betriebssystembestandteile Einfluss auf Scheduling-Entscheidungen. Beispielsweise Blocking-IO, IPC, Interrupts, Exceptione, etc. (Policy und Mechanisum macnhmal schwer zu trennen)
	\item \textbf{Herausforderungen}
	\begin{description}
		\item[Komplexität:] Sehr viele unterschiedliche Umgebungen, Policies, etc.
		\item[Leistungsauswirkungen:] Scheduling innerhalb des Systems verteilt und verwoben; viele beteiligte Komponenten; Trennung von Policy und Mechanismus eventuell komplex und ineffizient
	\end{description}
	\item \textbf{Scheduling Policies}
	\begin{description}
		\item[Time-based Scheduling (prozessorbasiert):] Abhängig von Working Set, Ausführungssignatur
		\item[Energy-aware Scheduling (prozessorbasiert):] Abhängig von Prozessorcharakterisitik (Performance-Counter, Sensoren, etc.)
		\item[Affinity Scheduling (ressourcenbasiert):] Vermeidung von teuren CPU-Wechseln, die Cache-Refills verursachen
		\item[Co-/Gang Scheduling:] Kommunikationsabhängig. Beispielsweise \texttt{IPC} oder Shared Memory
		\item[Load Balancing:] Lastverteilung in Abhängigkeit von Warteschlangenlänge, Idle-Time, Kontextwechselraten
	\end{description}
	\item \textbf{Ansätze}
	\begin{itemize}
		\item Kernel-Level Schedulung (traditionell)
		\begin{itemize}
			\item Kernel verantwortlich für Management/Scheduling/Dispatching der Threads; Ausführungskontext wird vom User initiiert \(\rightarrow\) globale Sicht/Zustand
			\item Kontext mit weiteren Abstraktionen verknüpft: Schutz des Speichers; Accounting; Kommunikation; Ressourcen
			\item Analyse
			\begin{itemize}
				\item Kernel kann Scheduling-Entscheidungen in allen Anwendungen durchsetzen; wenig Overhead, da zentraler Scheduling-State; allerdings eventuell problematisch, da eine einzige Scheduling-Strategie für alle Anwendungen (QoS schwierig)
				\item Komplexe, schwergewichtige Abstraktionen für Prozesswechsel-Kontext; Speicher-Kontext; I/O-Kontext; etc.
				\item Schwierig zu erweitern, da keine Modularisierung sowie tief in den Kernel angebettet
			\end{itemize}
		\end{itemize}
		\item Application-Level Scheduling (traditionell)
		\begin{itemize}
			\item Anwendungen selbst für Thread-Scheduling verantwortlich \(\rightarrow\) müssen Management/Scheduling/Dispatcher implementieren
			\item Keine Interaktion mit anderen Subsystemen/Schichten; gut erweiterbar
			\item Analyse
			\begin{itemize}
				\item Wenig Overhead durch Abstraktionen: Ledigleich innerhalb der Anwendungen; Thread-Wechsel verursacht nur einen Wechsel des Ausführungskontext
				\item Lokale Sicht, kann Anforderungen des Betriebssystems oder andere Anwendungen nicht beachten
			\end{itemize}
		\end{itemize}
		\item Fallstudie: Scheduler Activations
		\begin{itemize}
			\item Idee: Kernel-Level Threads sind zu schwergewichtig und zu wenig erweiterbar/anpassbar; User-Level Threads zu sehr anwendungsgebunden \(\rightarrow\) Kombination beider Ansätze
			\item Meist keine Intervention des Kernel beim Thread-Scheduling nötig (reine User-Level Threads). Kernel wird erst bei blockierten Threads oder Page Faults benötigt
			\item Voraussetzung: Verteiltes Scheduling, da die der Kernel den Anwendungszustand kennen muss (wie parallel arbeitet die Anwendung?) und die Anwendungen den Scheduling-Zustand des Kernels kennen muss (wenn blockiert ein Thread?)
			\item Abstraktion
			\begin{description}
				\item[Virtual Processors:] Kernel stellt Adressräumen virtuelle Multiprozessoren (VP) zur Verfügung. Adressräume können weitere VPs beim Kernel beantragen und diesen Threads zur Ausführung zuordnen
				\item[Scheduler:] Jeder Adressraum verfügt über einen dedizierten User-Level Scheduler
			\end{description}
			\item Mechanismen
			\begin{itemize}
				\item Scheduler benachrichtigt den Kernel bei Thread-Operationen, die den Prozessor betreffen
				\item Der Kernel benachricht den Scheduler bei allen Ereignissen, die den Adressspace betreffen
			\end{itemize}
			\item Zusammenfassung/Analyse
			\begin{itemize}
				\item Kombiniert Anwendungslevel- mit Kernel-Level-Scheduling
				\item Erweiterbar: Kernel lediglich für Dispatch verantwortlich, Anwender können beliebige Policies implementieren
				\item Nachteil: Bei Upcall jeweils zwei User-Kernel-Übergänge
			\end{itemize}
		\end{itemize}
		\item Fallstudie: K42 (Scheduler)
		\begin{itemize}
			\item K42: IBM Forschungsprojekt um ein allgemeines, modulares, Linux-kompatibles Betriebssystem zu entwickeln
			\item Scheduling zwischen Kernel und Anwendung ausgeteilt: Jeder Prozess besteht aus Adressraum und mindestens einem Dispatcher (vom Kernel geschedult, mehrere Dispatcher für Parallelität möglich, verschiedene Dispatcher können verschiedene Strategien implementieren)
			\item Dispatcher für Thread-Scheduling zuständig \(\rightarrow\) Anwendungen können eigene Thread-Modelle benutzen und beliebig viele Threads spawnen, ohne Kernel-Ressourcen verwenden zu müssen
			\item Resource-Domänen: Orthogonal zu Prozessen; haben eigene Ressourcen-Berechtigungen; können zum Accounting verwendet werden. Dispatcher sind jeweils an einen Prozessor gebunden und gehören zu einer \textit{Resource-Domäne} \(\rightarrow\) ermöglichen Ressourcenallokation für Mehrprozessanwendungen
		\end{itemize}
		\item Scheduling in Multi-Server Systemen
		\begin{itemize}
			\item Aufbau
			\begin{itemize}
				\item Threads repräsentieren verschiedene Entitäten (Servers, Clients, Ressourcen, etc.)
				\item Verwendung von IPC für jegliche interne Kommunikation (behandeln von Requests, Dispatching, Synchronisierung, Interrupts, etc.) \(\rightarrow\) eng mit Scheduling verknüpft
			\end{itemize}
			\item Szenarien
			\begin{itemize}
				\item % TODO
			\end{itemize}
			\item Probleme: Policy sollte von Anwendung gesteuert werden; IPC-Leistung kritisch
			\item Alternativen zu IPC: Policy-Upcalls (teuer); Kernel-Erweiterungen für Schedlung (sehr komplex, eventuell sicherheitskritisch); Heuristiken
		\end{itemize}
	\end{itemize}
\end{itemize}





















