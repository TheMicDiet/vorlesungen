\chapter{Parallelrechner und Parallelprogrammierung}

Zusammenfassung der Vorlesung "`Parallelrechner und Parallelprogrammierung"' aus dem Sommersemester 2017.\footnote{\url{https://www.scc.kit.edu/personen/11185.php}}



\section{Einführung}

\begin{itemize}
	\item \textbf{Klassifikation nach Flynn}
	\begin{description}
		\item[Single Instruction Single Data (SISD):] von-Neumann-Architektur (Einprozessorrechner)
		\item[Single Instruction Multiple Data (SIMD):] Vektorrechner
		\item[Multiple Instruction Single Data (MISD):] In der Praxis irrelevant. Ausnahme: Mehrere Geräte, die zur Berechnungsverifikation das selbe Datum mehrfach parallel berechnen
		\item[Multiple Instruction Multiple Data (MIMD):] Multiprozessorsystem
	\end{description}
	\item \textbf{Multiprozessorsysteme}
	\begin{description}
		\item[Speichergekoppelter:] Gemeinsamer Adresseraum; Kommunikation über gemeinsame Variablen; skalieren mit \(>1000\) Prozessoren
		\begin{description}
			\item[Uniform Memory Access Model (UMA):] Alle Prozessoren greifen gleichermaßen mit gleicher Zugriffszeit auf einen gemeinsamen Speicher zu (symmetrische Multiprozessoren)
			\item[Non-uniform Memory Access Modell (NUMA):] Speicherzugriffszeiten variieren, da Speicher physikalisch auf verschiedene Prozessoren verteilt ist (Distributed Shared Memory System (DSM))
		\end{description}
		\item[Nachrichtengekoppelt:] Lokale Adresseräume; Kommunikation über Nachrichten (No-remote Memory Access Model); theoretisch unbegrenzte Skalierung
		\begin{description}
			\item[Uniform Communication Architecture Model (UCA):] Einheitliche Nachrichtenübertragungszeit
			\item[Nin-uniform Communication Architecture Model (NUCA):] Unterschiedliche Nachrichtenübertragungszeiten in Abhängigkeit der beteiligten Prozessoren
		\end{description}
	\end{description}
\end{itemize}



\section{Parallelrechner}
\begin{itemize}
	\item \textbf{Leistungsfähigkeit}
	\begin{itemize}
		\item Maßzahl: Floating Point Operations per Second (FLOPS)
		\item Nicht notwendigerweise protportional zur Taktgeschwindkeit: Eventuell mehrere Zyklen zur Berechnung notwendig; Vektorprozessoren verarbeiten viele Floating Point Operations gleichzeitig (Grafikkarten)
		\item Messstandard: \texttt{LINPACK}-Benchmark
	\end{itemize}
\end{itemize}

\subsection{Shared Memory Multiprozessoren}
\begin{itemize}
	\item Programmierung durch effiziente automatische Parallelisierungswerkzeuge einfach und attraktiv
	\item \textbf{Cache-Kohärenz}
	\begin{itemize}
		\item Problem: Replikate in Caches verschiedener Prozessoren müssen kohärent gehalten werden (Lesezugriffe müssen immer den Wert des zeitlich letzten Schreibzugriffs liefern)
		\item Lösung: Verzicht auf strikte Konsistenz. Replikate müssen nicht zu jedem Zeitpunkt identisch sein
	\end{itemize}
\end{itemize}


\subsection{Distributed Memory Multiprozessoren}
\begin{itemize}
	\item Aufbau: Rechenknoten mit (mehreren) CPU(s), lokalem Speicher und ggf. lokaler Festplatte. Kopplung über Verbindungsnetzwerk
	\item Kriterien: Geschwindkeit, Parallelitätsgrad, Kosten, Latenz
\end{itemize}

\subsubsection{IBM RS/6000 SP (1990-2000)}
\begin{itemize}
	\item Nachrichtengekoppelter Multiprozessor mit \(2-512\) superskalaren, \(64\)-bit \texttt{POWER}-Knoten (\texttt{POWER3-II} weist \(1,5 GFLOPS\) Spitzenleistung auf)
	\item Verbindungsstruktur: High-Performance-Omega-Netzwerk (\(4 \times 4\)) bidirektionale Kreuzschinen
	\item Skalierbares IO-System über FileServer-Knoten
	\item Betriebssystem: \texttt{IBM AIX}
	\item \textbf{Aufbau}
	\begin{itemize}
		\item Frames: Jeweils \(16\) Knoten mit reduntanter Stromversorgung/Steuerungslogik sowie Hochleistungsnetzwerk
		\item High-Performance Switch
		\begin{itemize}
			\item Pro Frame ein \(16 \times 16\) Schaltnetzwerk, besteht aus \(4 \times 4\) Kreuzschinenschalter, zur Kommunikation mit anderen Frames
			\item \textit{Buffered Wormhole Routing}: Bei Kollisionen in den Verbindungselementen bleiben Nachrichten stehen und blockieren nachfolgende Datenpakete)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{IBM Power 4 in der IBM p690 eSeries 1600 Cluster}
\begin{itemize}
	\item \textbf{\texttt{POWER4}-Prozessor}
	\begin{itemize}
		\item Erster Dual-Core Prozessor mit gemeinsamen \texttt{L2}-Cache und \(10,4\) GFLOPS
		\item \textit{Fabric Controller} zum Zusammenschalten von mehreren Chips \(\rightarrow\) \textit{Multi-Chip-Module} (MCM)
		\begin{itemize}
			\item interne Kommunikation über unidirektionalen Ring mit \(40\) GBit/s
			\item Gemeinsames Memory-Subsystem für \(8-32\) GB
		\end{itemize}
		\item \textit{Central Electronics Complex} (CEC) zum Zusammenschalten von bis zu \(4\) MCMs und bis zu \(8\) Speicherkarten
	\end{itemize}
	\item \textbf{Logical Partitions (LPARs)}
	\begin{itemize}
		\item Partitionierung von Ressourcen in physikalische Maschinen
		\item Volle Flexibilität bei voller Isolation; jede Kopie führt \texttt{AIX/Linux} aus
	\end{itemize}
	\item Speicherkonfiguration pro Rack: Bis zu \(8\) Speicherslots
	\item Beispiel: Juelich-Multi-Processor (JUMP) mit \(41\) \texttt{p690+} Schränken
\end{itemize}


\subsection{Distributed Shared Memory Multiprozessoren}
\begin{itemize}
	\item \textbf{Überblick}
	\begin{itemize}
		\item Gemeinsamer Adressraum, einzelne Speichermodule sind auf die einzelnen Prozessoren verteilt
		\item Oft Cache-Kohärenz bei (lokalem) Speicherzugriffen
	\end{itemize}
	\item \textbf{Zugriff}
	\begin{itemize}
		\item Mikrobefehlsebene
		\begin{itemize}
			\item Transparent für das Maschinenprogramm
			\item Explizite Befehle für entfernten Speicherzugriff
		\end{itemize}
		\item Software DSM
		\begin{itemize}
			\item Jeder Prozessor kann immer auf gemeinsame Daten zugreifen. Synchronisation mittels Schloss,- Semaphor- oder Bedigungsvariablen
			\item DSM-System regelt Kommunikation selbständig über (zumeist) Message-Passing
			\item Vorteile: Entlastung des Programmierers; leichte Partierbarkeit von/zu eng gekoppelten Multiprozessoren
			\item Datenverwaltung
			\begin{description}
				\item[Seitenbasiert:] Nutzung der virtuellen Speicherverwaltung des Betriebssystems zur expliziten Platzierung der Daten (unterschiedliche Granularität der Seiten: \(16\) Byte bis \(8\) kByte)
				\item[Objektbasiert:] % TODO
			\end{description}
			\item Probleme bei seitenbasierter Datenverwaltung
			\begin{itemize}
				\item Geringe Effizienz beim Nachladen über das Verbindungsnetz
				\item Lineares, strukturloses Feld von Speicherwort
				\item False Sharing und Flattern (Trashing)
				\begin{itemize}
					\item False Sharing: Eine Speicherseite beinhaltet mehrere Datenwörter, die von verschiedenen Prozessoren benötigt werden (Kohärenz auf Seitenebene) \(\rightarrow\) nach jedem Schreizugriff eines Datenwortes muss die komplette Seite neu zu den anderen Prozessoren übertragen werden
					\item Flattern (Trashing): Bei mehrfachen Schreibzugriffen wird die Seite immer wieder übertragen
					\item Gegenmaßnahmen
					\begin{itemize}
						\item Verkleinerung der Seitengröße. Allerdings steigt damit der Seitenverwaltungsaufwand
						\item Objektbasiertes Software SDM-System: Gemeinsame Variablenzugriffe werden vom Precompiler erkannt und durch Bibliotheksfunktionen für entfernte Zugriffe ersetzt \(\rightarrow\) es werden nur Datenobjekte verschoeben, die benötigt werden \(\rightarrow\) \textit{False Sharing} wird ausgeschlossen
					\end{itemize}
				\end{itemize}
			\end{itemize}
			\item Datenlokation und Datenzugriff
			\begin{itemize}
				\item Jeder Knoten muss Daten finden können (Datenlokation) und darauf zugreifen (Datenzugriff)
				\item Statische Verwaltung der Daten
				\begin{itemize}
					\item Daten werden zentral auf einem oder mehreren Servern verwaltet (Verteilung wird nicht verändert)
					\item Datenlokalisierung funktionsbasiert
					\item Konsistenz durch Sequentialisierung auf dem zuständigen Server
				\end{itemize}
				\item Dynamische Verwaltung der Daten
				\begin{itemize}
					\item Datum wird vor Zugriff auf zugreifenden Knoten verschoben \(\rightarrow\) alle Zugriffe sind lokal (single-reader-single-writer-Konzept)
					\item \textit{False Sharing} bei seitenbasiertem System
					\item Konsistenz durch Verschieben der Seiten
				\end{itemize}
			\end{itemize}
			\item Replizierte Daten
			\begin{itemize}
				\item Bisher: Knoten können nur sequentiell auf Daten zugreifen
				\item Replikation ähnelt Caching
				\begin{description}
					\item[Lesereplikation:] Reine Lesekopie, kann nicht geändert und zurückgeschrieben werden
					\item[Leseanfrage:] Falls vorhanden, Verwerfen einer Schreibkopie; danach Anfordern einer neuen Lesekopie
					\item[Schreibanfrage:] Verwerfen aller Kopien; danach Anlegen der Schreibkopie so wie zurückschreiben
				\end{description}
				\item Volle Replikation: multiple readers/multiple writers
				\begin{itemize}
					\item Jeder Knoten kann lokal Änderungen vornehmen \(\rightarrow\) Konsistenz schwierig
					\item Ansatz potentiell am effizientesten, da alle Zugriffe lokal sind
				\end{itemize}
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Beispielsystem: Cray T3E}
\begin{itemize}
	\item NCC-NUMA-Konzept: Puffern von Cache-Blöcken nur bei prozessorlokalen Speicherzugriffen (keine Kohärenz über alle Cache-Speicher)
	\item Erster Supercomputer, der \(>1\) TFLOPS bei einer wissenschaftlichen Anwendung erzielte (1998)
	\item Verwendung von \(8-2176\) Alpha-Mikroprozessoren mit 3D-Torus-Nutzwerk
	\item \textbf{Adressierung}
	\begin{itemize}
		\item Aufbau einer globalen Adresse: Verarbeitungselementnummer (PE-Nummer) und lokaler Offset
		\item Adressumsetzung in Hardware (virtuelle \(\rightarrow\) logische \(\rightarrow\) physische Adresse)
	\end{itemize}
	\item \textbf{Verbindugnsnetzwerk}
	\begin{itemize}
		\item Dreidimensionaler Torus (dreidimensionales, ringförmig geschlossenes Gitter)\footnote{\url{https://en.wikipedia.org/wiki/Torus_interconnect}}
		\item Daten können gleichzeitig über separate Kommunikationspfade ohne Beteiligung der Verarbeitungselemente (unabhängige Hardware-Unterstützung für den Nachrichtenaustausch) in alle drei Richtungen transportiert werden \(\rightarrow\) kurze Verbindungswege, schnelle Übertragung
	\end{itemize}
	\item \textbf{Mechanismen zur Synchronisation}
	\begin{description}
		\item[Barrier-Synchronisation:] Barrier-Modus (\texttt{UND}-Verknüpfung) und Eureka-Modus (\texttt{ODER}-Verknüpfung)
		\item[Fetch-and-Increment:] Atomares Lesen eines Werts und Inkrementieren eines speziellen lokalen Registers
		\item[Atomic-Swap:] Atomares Tauschen von lokalem Registerinhalt mit dem Inhalt einer (entfernten) Speicherzelle
	\end{description}
	\item Betriebssystem: Microkernel pro Verarbeitungselement
\end{itemize}

\subsubsection{Beispielsystem: SGI Altix (LRZ)}
\begin{itemize}
	\item CC-NUMA auf Basis von Intel Itanium Dual-Cores mit insgesamt \(9728\) cores (Platz 10, 06/2007)
	\item Leistung: \(56,5\) TFLOS (Linpack) bei ca. \(1\) MW Energiebedarf
	\item Nachfolgesystem \texttt{SuperMUC}: \(3,2\) PFLOPS bei \(2\) MW Energiebedarf
\end{itemize}


\subsection{Cluster Systeme}
\begin{itemize}
	\item Hintergrund:Zu Beginn häufig Spezialprozessoren in Supercomputern (Consumermarktentwicklung noch zu langsam). Später vermehrt Nutzung von \texttt{x86}-Standardprozessoren
	\item \textbf{Charakterisierung}
	\begin{itemize}
		\item Jeder Knoten als eigenes System mit eigenem OS
		\item Nachrichtenaustausch früher: Interprozesskommunikation über Netzwerk
		\item Nachrichtenaustausch heute: Cluster sind durch Multicore Prozessoren hybride Systeme
		\begin{itemize}
			\item Gemeinsamer Speicher in einem Knoten (OpenMP)
			\item Verteilter Speicher zwischen den Knoten (MPI)
			\item Somit zwei Parallelitätsebenen
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{HPC Cluster am KIT}
\begin{itemize}
	\item \textbf{HC3: HP XC3000 (seit Februar 2010)}
	\begin{itemize}
		\item \(356\) Rechenknoten mit jeweils zwei \texttt{Intel Xeon E5540} Quad Core (\(27,04\) TFLOPS Spitzenleistung) und insgesamt \(\approx 10\) TB Hauptspeicher
		\item OS: \texttt{HP XC Linux}
		\item Infiniband Netzwerk
		\item Nutzung durch KIT Institute
	\end{itemize}
	\item \textbf{InstitutsCluster 2 (IC2)}
	\begin{itemize}
		\item \(6560\) Xeon-Cores mit \(135,7\) TFLOPS Spitzenleistung und \(\approx 28\) TB Hauptspeicher
		\item OS: \texttt{SLES 11} mit \texttt{KITE} Managementsoftware
		\item Infiniband Netzwerk
		\item Nutzung durch KIT Institute
	\end{itemize}
	\item \textbf{bwUniCluster}
	\begin{itemize}
		\item Gemeinsames System aller Lindesuniversitäten Baden Württembergs
		\item Systemarchitektur: \texttt{64bit x86} MultiCore-Prozessoren (sandy bridge); Infiniband FDR (\(56\) GBit/s)
		\item Kennzahlen Cluster: \(8448\) Cores mit \(175\) TFLOPS Spitzenleistung bei \(\approx 200\) KW Energieverbrauch
		\item Kennzahlen Speichersystem: \(900\) TB Speicher bei \(\approx 20\) KW Energieverbauch
	\end{itemize}
	\item \textbf{ForHLR Stufe 1}
	\begin{itemize}
		\item Landeshöchstleistungsrechner: System der Ebene Tier-2; Rechenzeitvergabe nach wissenschaftlichem Begutachtungsprozess
		\item \(528\) Xeon Prozessoren mit insgesamt \(10752\) Cores, \(\approx 41\) TB Hauptspeicher und \(232\) TFLOPS Spitzenleistung bei einem Energieverbauch von \(226\) KW
		\item Infiniband 4X FDR
		\item Benchmarks
		\begin{description}
			\item[Top500:] 243 (06/2014)
			\item[Green500:] 69 (06/2014)
		\end{description}
	\end{itemize}
	\item \textbf{ForHLR Stufe 2}
	\begin{itemize}
		\item \(1152\) HPC-Knoten mit jeweils zwei DecaCore \texttt{Intel Xeon E5-2660v3}, \(64\) GB Hauptspeicher und \(480\) GB SSD
		\item Spitzenleistung: \(24.048\) Cores mit \(95\) TB Hauptspeicher und \(1,1\) PFLOPS Spitzenleistung
		\item OS: \texttt{RHEL 7.x} mit \texttt{KITE} Managementsoftware
		\item Heißwasserkühlung mit \(\approx 40^\circ\) Grad Vorlauftemperatur und \(> 45^\circ\) Grad Rücklauftemperatur \(\rightarrow\) freie Kühlung selbst bei tropischen Temperaturen
		\item Visualisierungskomponenten
		\begin{itemize}
			\item \(21\) Visualisierungsknoten mit jeweils \(4\) Dodeka-Core \texttt{Intel Xeon E7-4830v3}, \(1\) TB Hauptspeicher, \(4\) \(960\) GB SSDs und \(4\) \texttt{NVIDIA GeForce GTX980 Ti}
			\item Neues Visualisierungslabor mit zwei \texttt{4K} Projektoren
		\end{itemize}
		\item Dateisystem und Integration
		\begin{itemize}
			\item \(4,8\) PB bei \(80\) GB/s Durchsatz
			\item Kopplung der Dateisysteme von \texttt{ForHLR I} (Campus Süd) und \texttt{ForHLR II} (Campus Nord) über redundante \(320\) GBit/s Infiniband-Leitung (\(35\) km Laufstrecke)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Jülich Reasearch on Petaflop Architectures (JUROPA)}
\begin{itemize}
	\item \textbf{Aufbau}
	\begin{itemize}
		\item \texttt{Sun Blade 6048 System} mit \(2208\) Rechenknoten, \(17664\) Cores und \(\approx 52\) TB Hauptspeicher
		\item Pro Rechenknoten: Zwei \texttt{Intel Xeon X5570} QuadCore mit \(24\) GB Hauptspeicher
		\item Gesamtleistung: \(207\) TFLOPS Spitzenleistung
		\item Netzwerk: Infiniband QDR (non-blocking Fat Tree)
	\end{itemize}
	\item \textbf{Schwestersystem}
	\begin{itemize}
		\item \(8640\) Cores mit \(101\) FTLOPS Spitzenleistung
		\item Exklusiv für europäische Fusions-Wissenschaften
	\end{itemize}
	\item Besonderheiten der Systeme: Unterschiedliche Hersteller aber mit gleicher/kompatibler Hardware. Werden gemeinsam Entwickelt und können als ein Gesamtsystem betrieben werden
\end{itemize}


\subsection{Vektorrechner}
\begin{itemize}
	\item Rechner mit pipelineartig aufgebautem Rechenwerk zur Verarbeitung von Vektoren von Gleitkommazahlen
	\item Skalarverarbeitung: Verknüpfung von Vektoren mit einzelnen Operanden
	\item \textbf{Beispiel: Vektor-Addition in vierstufiger Pipeline}
	\begin{description}
		\item[Stufe 1:] Laden von Gleitkommazahlen aus dem Vektorregister
		\item[Stufe 2:] Exponenten vergleichen und Mantisse verschieben
		\item[Stufe 3:] Verschieben der ausgerichteten Mantisse
		\item[Stufe 4:] Ergebnis normalisieren und zurückschreiben
	\end{description}
	\item \textit{Chaining}: Verketten von (spezialisierten) Pipelines. Ergebnisse der vorderen Pipeline werden direkt der nächsten Pipeline zur Verfügung gestellt
	\item \textbf{Möglichkeiten zur Parallelisierung}
	\begin{description}
		\item[Vektor-Pipeline-Parallelität:] Durch die Pipeline selbst gegeben
		\item[Mehrere Pipelines pro Vektoreinheit:] Parallelität durch \textit{Chaining} von Pipelines
		\item[Vervielfachen der Pipelines:] Verwendung mehrerer parallel arbeitender, gleichartiger Pipelines
		\item[Mehrere Vektoreinheiten:] Mehrere unabhängige Vektoreinheiten
	\end{description}
	\item Vektorrechner heutzutage: Keine Relevanz mehr, da Standardprozessoren \texttt{SIMD}-Operationen effizient ausführen können
\end{itemize}

\subsubsection{NEC SX Serie}
\begin{itemize}
	\item \textbf{Beispielsystem am SCC (NEC SX-9)}
	\begin{itemize}
		\item Ein \texttt{SX-9} Knoten enthält \(16\) Vektorprozessoren (mit jeweils \(8\) Vektorpipelines) mit \(1\) TB Hauptspeicher und \(1,6\) TFLOPS Spitzenleistung
		\item Sehr gut geeignet für Strömungsberechnungen
		\item Größtmögliches System: \(512\) Knoten mit \(970\) TFLOPS Spitzenleistung
	\end{itemize}
	\item \textbf{Weitere \texttt{SX-9} Systeme}
	\begin{itemize}
		\item HLRS Stuttgart: \(12\) Knoten mit \(19,2\) TFLOPS Spitzenleistung
		\item Der Deutsche Wetterdienst besitze zwei unabhängige Systeme mit jeweils \(14\) Knoten und \(23\) TFLOPS Spitzenleistung
	\end{itemize}
\end{itemize}


\subsection{Ressourcenmanagement}
\begin{itemize}
	\item Typischerweise Knappheit von Parallelrechnerressourcen \(\rightarrow\) Ressourcenmanagement/Job Scheduling notwendig
	\item \textbf{Job Scheduling}
	\begin{enumerate}
		\item[Time-Sharing:] Simultane Ausführung mehrerer Jobs auf der gleichen Ressource nach einem \textit{Round-Robin}-Verfahren. Anwendung bei PCs
		\item[Space-Sharing:] Jobs bekommen exklusiv Ressourcen zur Ausführung zugewiesen, müssen allerdings ggf. warten bis genügend Ressourcen frei sind (Batch-System). Laufzeiten müssen abgeschätzt werden, meist gibt es Obergrenzen. Anwendung bei Clustern/Supercomputern
	\end{enumerate}
	\item \textbf{Beispiel: LoadLeveler}
	\begin{itemize}
		\item Job Scheduler von \texttt{IBM} zum Ausführen von Batch-Jobs
		\item Grundlegende Befehle zum Hinzufügen/Anzeigen/Abbrechen/Status anzeigen von Jobs
		\item Hinzufügen von Jobs per Job-Kommando-Datei (\textit{cmdfile})
	\end{itemize}
	\item \textbf{Beispiel: Torque (Resource Manager) + Maui (Cluster Scheduler)}
	\begin{itemize}
		\item Grundlegende Befehle zum Hinzufügen/Anzeigen/Abbrechen/Status anzeigen von Jobs
		\item Hinzufügen neuer Jobs per Kommandozeile
		\item Beispiel: \texttt{msub -l nodes=32:ppn=2,pmem=1800mb,walltime=3600 myscript}
	\end{itemize}
\end{itemize}



\section{Programmiermodelle und Grundlagen}

\subsection{Parallelitätsebenen/Parallelitätstechniken}
\begin{itemize}
	\item \textbf{Ebenen der Parallelität}
	\begin{enumerate}
		\item Programmebene (Jobebene): Beispielsweise gleichzeitig, von einander unabhängige Programme auf einem Betriebssystem
		\item Prozessebene (Taskebene): Beispielsweise Linux-Prozesse eines Programms, die auf gemeinsamen Daten arbeiten
		\item Blockebene (Threadebene): Beispielsweise parallel ausgeführte Schleifeniteration
		\item Anweisungsebene: Beispielsweise einzelne Maschinenbefehle, die parallel ausgeführt werden (beispielsweise bei superskalaren Prozessoren oder VLIW) 
		\item Suboperationsebene: Beispielsweise Vektorbefehle in einer Vektropipeline; setzt vektorisierende Compiler und komplexe Datenstrukturen voraus
	\end{enumerate}
	\item \textbf{Körnigkeit/Granularität}
	\begin{itemize}
		\item Verhältnis von Rechenaufwand zu Kommunikations-/Synchronisationsaufwand
		\item Klassifizierung
		\begin{description}
			\item[Grobkörnig:] Programm-, Prozess- und Blockebene
			\item[Mittelkörnig:] Blockebene, jedoch selten verwendet
			\item[Feinkörnig:] Anweisungsebene
			\item[Noch feinkörniger:] Supoperationsebene
		\end{description}
	\end{itemize}
\end{itemize}


\subsection{Maschinenmodelle}
\begin{itemize}
	\item \textbf{Random-Access Machine (RAM)}
	\begin{itemize}
		\item Einprozessorrechner mit jeweils einmal: Recheneinheit, Programm, RW-Speicher, Eingabeband, Ausgabeband
		\item Befehle mit Zielregister und zwei Operandenregistern
	\end{itemize}
\end{itemize}

\subsubsection{Parallel Random-Access Machine (PRAM)}
\begin{itemize}
	\item Modell eines idealen speichergekoppelten Parallelrechners. Keine Beachtung von Synchronisations-/Zugriffskosten
	\item \texttt{n}-Prozessor-PRAMs bestehen aus \texttt{n} identischen Prozessoren, mit gemeinsamen Speicher/Takt
	\item Praktische Bedeutung: PRAMs idealisieren Parallelrechner (s.o.) ohne Beachtung der Lokalität des global adressierbaren Speichers (Zugriff aller Prozessoren auf jede Speicherzelle immer in Einheitszeit)
	\item \textbf{Zugriffskonflikte}
	\begin{itemize}
		\item Typen von Zugriffskonflikten
		\begin{description}
			\item[Exclusive Read (ER):] Pro Zyklus kann höchstens ein Prozessor die selbe Speicherzelle lesen
			\item[Exclusive Write (EW):] Pro Zyklus kann höchstens ein Prozessor die selbe Speicherzelle beschreiben
			\item[Concurrent Read (CR):] Pro Zyklus können mehrere Prozessoren die selbe Speicherzelle lesen
			\item[Concurrent Write (CW):] Pro Zyklus können mehrere Prozessoren die selbe Speicherzelle beschreiben \(\rightarrow\) Schreibkonflikte müssen gelöst werden
		\end{description}
		\item Kombinationen von Zugriffskonflikten
		\begin{description}
			\item[EREW-PRAM:] Keine gemeinsamen Zugriffe
			\item[CWER-PRAM:] Gemeinsames Lesen erlaubt, gemeinsames Schreiben verboten
			\item[ERCW-PRAM:] Exklusiver Lesezugriff, gemeinsamer Schreibzugriff
			\item[CRCW-PRAM:] Gemeinsame Lese-/Schreibzugriffe erlaubt
		\end{description}
		\item Lösen von Schreibkonflikten
		\begin{description}
			\item[Common (C-CRCW):] Gemeinsamer Schreibzugriff nur erlaubt, wenn alle Prozessoren den selben Wert schreiben wollen
			\item[Arbitrary (A-CRCW):] Ein Prozessor gewinnt, alle anderen werden ignoriert
			\item[Priority (P-CRCW):] Der Prozessor mit dem kleinsten Index gewinnt
		\end{description}
	\end{itemize}
	\item Zusammenfassung: Speichergekoppelt; taktsynchron; gut zu programmieren
\end{itemize}

\subsubsection{Bulk Synchronous Parallel (BSP)}
\begin{itemize}
	\item Nachrichtengekoppeltes Maschinenmodell
	\item Aufbau: Prozessoren (mit oder ohne lokalem Speicher); Kommunikationsnetz; Barrierensynchronisation
	\item Definiert über Anzahl der Prozessoren \(p\), Kommunikationskostenfaktor \(g\) und die minimale Zeit für die Ausführung eines Superschrittes \(L\)
	\item \textit{Super-Step} Betrieb
	\begin{itemize}
		\item Sequentielle Ausführung von Superschritten. Pro Superschritt arbeiten die Prozessoren unabhängig von einander
		\item Bestandteile pro Superstep
		\begin{itemize}
			\item Berechnungsschritte auf lokalen Variablen, die zu Beginn des Superschritts zur Verfügung stehen
			\item Senden/Empfangen von Nachrichten
			\item Am Ende: Barrierensynchronisation aller Prozessoren vor dem nächsten Superschritt
		\end{itemize}
	\end{itemize}
	\item Zusammenfassung: Nahrichtengekoppelt, Superschritt-Synchronisation
\end{itemize}

\subsubsection{LogP-Modell}
\begin{itemize}
	\item Prozessoren arbeiten unabhängig und tauschen Nachrichten aus
	\item \textbf{Definition einer Maschine}
	\begin{description}
		\item[L:] Maximale Latenz einer Übertragung einer kurzen Nachricht (weniger Wörter umfassend)
		\item[o:] Zeitbedarf zum Übertragen der Nachricht. Keine weitere Befehlsausführung während Senden oder Empfangen
		\item[g:] Untere Zeitschranke, die pro Prozessor zwischen zwei Übertragungen eingehalten werden muss
		\item[P:] Anzahl Prozessoren mit optionalem lokalen Speicher
	\end{description}
	\item Endliche Netzwerkbandbreite: Es können immer nur \(\frac{L}{g}\) Nachricht zeitgleich ausgetauscht werden
	\item \(L\) ist obere Schranke pro Nachrichtenübertragung aufzufassen
	\item Zusammenfassung: Nachrichtengekoppelt; explizite Kommunikation; geeignet für Laufzeitabschätzungen
\end{itemize}


\subsection{Quantitative Maßzahlen für Parallelrechner}
\begin{itemize}
	\item Prozessorzustände während Programmausführung: \textit{rechnend}, \textit{kommunizierend}, \textit{wartend} (auf Kommunikation)
	\item \textbf{Laufzeitmessung von \(T(1)\)}
	\begin{description}
		\item[Relative Beschleunigung (Algorithmenabhängig):] Paralleler Algorithmus wird auf Multiprozessorsystem so ausgeführt, als sei er sequentiell
		\item[Absolute Beschleunigung (Algorithmenunabhängig):] Verwenden des besten bekannten sequentiellen Algorithmus
	\end{description}
	\item \textbf{Skalierbarkeit}
	\begin{itemize}
		\item Definition: Das Hinzufügen von Verarbeitungselementen führt zur Verkürzung der Ausführungszeit, ohne dass das Programm geändert wird
		\item Annahme: Konstante Problemgröße
		\item Skalierung ist immer beschränkt: Sättigung ab einer bestimmten Prozessorzahl
		\item Skaliert ein System, so tritt das Problem bei einer wachsenden Problemgröße nicht auf
	\end{itemize}
	\item \textbf{Amdahls Gesetz}
	\begin{itemize}
		\item Aufteilung eines Programms in einen parallelen und einen sequentiellen Anteil. Speedup-Berechnung unter der Annahme, das kein Programm komplett parallelisiert werden kann
		\item Es gilt: \(1 \le S(p) \le p\)
		\item Superlinearer Speedup: \(S(p) > p\)
		\begin{itemize}
			\item Tritt nur selten auf, sorgt für Konfusion
			\item Gründe: Cache-Effekte durch die Speicherhierarchie; mit steigender Prozessorzahl steigt wächst die Cache-Hierarchie \(\rightarrow\) Reduzierung der Hauptspeicherzugriffe \(\rightarrow\) kürzere Laufzeit bei paralleler Ausführung
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Prozesse und Threads}
\begin{itemize}
	\item Parallelität ist eine spezielle Form von Nebenläufigkeit. Nebenläufige Anweisungen sind nur dann parallel, wenn zur Ausführung mehrere Prozessoren zur Verfügung stehen
	\item \textbf{Definitionen}
	\begin{description}
		\item[Prozessumgebung:] Geschützter Adressbereich eines Prozesses mit individuellem Code-/Datenbereich im Speicher \(\rightarrow\) Prozesswechsel bedingt somit (i.d.R.) einen Umgebungswechsel
		\item[Prozesskontext:] Registerwerte des Prozessors während der Prozessausführung. Beeinhaltet u.a. Befehlszähler, Stackpointer, etc. \(\rightarrow\) Prozesswechsel bedingt immer auch einen Kontextwechsel
	\end{description}
	\item Laufzeitbetrachtung: Erheblicher Aufwand durch Prozesserzeugung; Zugriffsschutzmechanismen (wenn Prozesse mehrerer Benutzer ausgeführt werden); Prozesswechsel; Kommunikations-/Synchronisationsoperationen
	\item \textbf{Prozesstypen}
	\begin{description}
		\item[Schwergewichtige Prozesse (Prozesse):] Klassische Prozesse
		\item[Leichtgewichtige Prozesse (Threads):] Mehrere \textit{Threads} teilen sich eine gemeinsame Prozessumgebung (Adressraum, Filepointer, geschützten Speicherbereich, etc.). Threadwechsel erzeugen keinen Umgebungswechsel, lediglich einen Kontextwechsel
	\end{description}
\end{itemize}


\subsection{Synchronisation und Kommunikation über gemeinsame Variablen}
\begin{itemize}
	\item \textbf{Synchronisation}
	\begin{itemize}
		\item Koordination zwischen Prozessen/Threads
		\item Einseitige Synchronisation: Abhängige Aktivitäten müssen in der richtigen Reichenfolge ausgeführt werden
		\item Mehrseitige Synchronisation: Zeitliche Reihenfolge der Ausführung unerheblich, Ausführung darf allerdings nicht parallel erfolgen (gemeinsamer Ressourcenzugriff) \(\rightarrow\) \textit{Mutual Exclusion} (gegenseitiger Ausschluss)
		\item \textit{Critical Section}: Anweisungen, deren Ausführung einen gegenseitigen Ausschluss erfordern
	\end{itemize}
	\item \textbf{Deadlocks (Verklemmungen)}
	\begin{itemize}
		\item Deadlock: Prozesse warten auf Ereignisse, die nicht mehr eintreten können \(\rightarrow\) Prozess wird undefinierbar verzögert (Aussperrung)
		\item Voraussetzungen für Deadlocks
		\begin{enumerate}
			\item Mutual Excklusion: Betriebsmittel sind nur exklusiv nutzbar
			\item No Preemption: Betriebsmittel können nicht entzogen werden
			\item Hold and Wait: Es werden Betriebsmittel gehalten während auf andere Betriebsmittel gewartet wird
			\item Circular Wait: Zirkuläre Abhängigkeit von Betriebsmittel von mindestens zwei Prozessen
		\end{enumerate}
		\item Vermeidung von Deadlocks
		\begin{itemize}
			\item Schlossvariable
			\begin{itemize}
				\item Abstrakte Datenstruktur, auf die alle Prozesse zugreifen müssen, die einen kritischen Abschnitt betreten wollen
				\item Operationen
				\begin{description}
					\item[lock:] Verschließt den kritischen Abschnitt von innen. Prozesse warten bis der kritische Abschnitt frei ist. Ist selbst ein kritischer Abschnitt \(\rightarrow\) Implementierung über atomare Befehle (\texttt{TEST\_AND\_SET}, \texttt{RESET}) oder der spezielle Maschineninstruktion \texttt{swap} (tauscht den Inhalt eines Registers und eines Speicherplatzes ohne untebrochen werden zu können). \textit{Spinlock} zum zyklischen Prüfen, ob ein Bereich wieder frei ist (ebenfalls per \texttt{swap})
					\item[unlock:] Gibt den Bereich wieder frei
				\end{description}
			\end{itemize}
			\item Semaphor
			\begin{itemize}
				\item Aufbau: Nicht-negative Integer-Variable; atomare Operationen "`passieren"' und "`verlassen"'
				\item Initialisierung über maximale Anzahl Prozesse, die eine Ressource zeitgleich nutzen dürfen. Bei Betreten wird der interne Zähler dekrementiert, beim Verlassen inkrementiert. Ist der Zähler Null, so muss auf Freigabe durch einen anderen Prozess gewartet werden
				\item Spezialfall: \texttt{new Semaphore(1)} wird als binäre Semaphore/Mutex Lock bezeichnet
				\item Nachteil: Kann bei fehlerhafter Implementierung (Vergessen von \textit{Verlassen}) das ganze System lahmlegen
				\item Unterschied zu \textit{Schlossvariable}: \textit{Verlassen} kann von allen Prozessen/Threads aufgerufen werden
			\end{itemize}
			\item Kritischer Abschnitt
			\begin{itemize}
				\item Zu jedem Zeitpunkt darf sich höchstens ein Prozess/Thread im kritischen Abschnitt befinden
				\item Bedingter kritischer Abschnitt: Kritischer Abschnitt kann nur betreten werden, wenn die Bedingung wahr ist (Implementierung von Fairness/Prioritäten/etc.)
				\item Wird der kritische Bereich freigegeben, bewerben sich alle wartenden Prozesse (bei denen die Bedingung wahr ist) um den kritischen Bereich
			\end{itemize}
			\item Monitor
			\begin{itemize}
				\item Zugriff exklsuiv einem Prozess vorbehalten. Abstrahiert die Zugriffsfunktionen/Realisierung des gegenseitigen Ausschlusses
				\item Im Monitor vereinbarte Variablen sind exklusiv und können nie gleichzeitig benutzt werden
				\item Innerhalb des Monitors haben Funktionen ledigleich Zugriff auf Monitorvariablen
				\item Auf monitorgebundene Variablen kann von außen nicht zugegriffen werden
				\item \textit{Verlassen} kann nur innerhalb des Monitors aufgerufen werden
				\item Vorteil gegenüber Semaphor: Kann nicht durch weitere hinzugekommene Anwenderprozesse gestört werden, wenn einmal korrekt programmiert
				\item Vorteil gegenüber bedingtem kritischen Abschnitt: Kann nicht nur Befehlsfolgen sondern auch Funktionen enthalten
			\end{itemize}
			\item Barriere
			\begin{itemize}
				\item Synchronisationspunkt für mehrere Prozesse
				\item Verwendung
				\begin{itemize}
					\item Initialisierung von Barrier-Variable mit Anzahl der Prozesse, auf die gewartet werden muss
					\item \texttt{wait-barrier} muss von jedem dieser Prozesse ausgeführt werden. Dabei wird der Zahler der Barriere dekrementiert bis sie Null ist
				\end{itemize}
				\item Verbindungsstruktur von Parallelrechner werden zur Implementierung sehr schneller Barriere-Sync-Operationen genutzt
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Kommunikation über Nachrichten}
\begin{itemize}
	\item Verbrauchbarkeit: Nachrichten existieren nur für die Zeit des Übertragungsvorgangs
	\item Sequentialbeziehung (implizite Synchronisation): Nachrichten können erst nach dem Senden empfangen werden
	\item Bestandteile einer Nachticht: Ziel; Nummer; Speicherbereich, der verschickt werden soll; Anzahl und Datentyp der Datenelemente im Sendepuffer
	\item Sendeoptionen: Asynchron/Synchron; Gepuffert/Ungepuffert; Blockierend/Unblockierend
	\item Empfangsoptionen: Konsumierend (zerstörend)/konservierend; Asynchron/Synchron
	\item Adressierungsarten: Direkte Bennenung; Briefkasten; Port (global bekannt); Verbindung/Kanal (lokal in Prozessen vereinbart)
\end{itemize}


\subsection{Parallele Programmstrukturen}
\begin{itemize}
	\item \textbf{Fork-Join Modell}
	\begin{itemize}
		\item Prozesse werden zu Beginn eines parallelen Codeteils durch \texttt{fork}-Operationen erzeugt
		\item Diese arbeiten unabhängig weiter und erreichen am Ende eine \texttt{join}-Operation zur Synchronisation mit Erzeugerthread (\texttt{T1})
		\item Nachteil: Aufwand zur Threaderzeugung
	\end{itemize}
	\item \textbf{SPMD Modell}
	\begin{itemize}
		\item Ziel: Vermeidung des Aufwands zur Thread-Ezeugung \(\rightarrow\) Threads werden nur einmal beim Programmstart erzeugt und terminieren erst am Programmende
		\item Sequentielle Codeteile werden von allen Prozessen ausgeführt; parallele individuell
	\end{itemize}
	\item \textbf{Reusable-Thread-Pool Modell}
	\begin{itemize}
		\item Idee: Kombinieren von Fork-Join Modell (Vermeiden der Mehrfachgenerierung der Prozesse) und SPMD Modell (Vermeidung der Mehrfachausführung der sequentiellen Codeteilen)
		\item Threads werden nur einmal erzeugt (wenn sie benötigt werden) und "`schlafen gelegt"', wenn sie gerade nicht gebraucht werden \(\rightarrow\) sequentieller Code wird nur einmal ausgeführt
	\end{itemize}
\end{itemize}



\newpage
\section{Appendix A: Formelsammung}

\subsection{Parallelrechner}

\subsubsection{Ende-zu-Ende Übertragungszeit}
Overhead: Zeit, um Nachrichten in/aus dem Netzwerk zu bekommen
\begin{equation}
	latency = t_{overhead} + t_{routing} + t_{transmission} + t_{contention}
\end{equation}


\subsection{Programmiermodelle und Grundlagen}

\subsubsection{Ausführungszeit}
\begin{equation}
	t = t_{computaton} + t_{communication} + t_{idle}
\end{equation}

\subsubsection{Übertragungszeit einer Nachricht}
\begin{equation}
	t_{msg} = t_{startup} + n \cdot \big( t_{transfer} \big)
\end{equation}

\subsubsection{Beschleunigung (Speedup)}
\begin{equation}
	S(p) = \frac{T(1)}{T(p)}
\end{equation}

\subsubsection{Effizienz}
\begin{equation}
	E(p) = \frac{S(p)}{p}
\end{equation}

\subsubsection{Amdahls Gesetz}
\begin{equation}
	T(p) = \underbrace{T(1) \cdot \frac{q}{p}}_{Paralleler~Anteil} + \underbrace{T(1) \cdot \big(1-q\big)}_{Sequentieller~Anteil}
\end{equation}
\begin{equation}
	S(p) = \frac{T(1)}{T(p)} = \frac{T(1)}{T(1) \cdot \frac{q}{p} + T(1) \cdot \big(1-q\big)} = \frac{1}{\frac{q}{p} + \big( 1 - q \big)} \le \frac{1}{1-q}
\end{equation}